{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "module_path ='/home/Mingke.Chen/OwO/hh'\n",
    "sys.path.append(module_path)\n",
    "import copy\n",
    "from tensorflow.contrib import slim\n",
    "import tensorflow as tf\n",
    "#from nets import inception_v4\n",
    "from nets import alexnet\n",
    "from tensorflow.python.ops import init_ops\n",
    "tf.logging.set_verbosity('ERROR')\n",
    "from nets.batch_data_generater_hson import batch_data_generater\n",
    "from nets.batch_data_generater_hson_valid import batch_data_generater as batch_data_generater_valid\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "class VeriSee_training():\n",
    "    def __init__(self,logdir,model_name):\n",
    "        self.init_checking_point_and_log_dir(logdir,model_name)\n",
    "\n",
    "    \n",
    "        X_data=tf.placeholder(tf.float32, (None, 64, 64, 3))\n",
    "        y_label=tf.placeholder(tf.float32, (None,3))\n",
    "        self.X_data=X_data\n",
    "        self.y_label=y_label\n",
    "\n",
    "        \n",
    "        self.init_training_parameter()\n",
    "        \n",
    "        self.predict_tensor,self.sess=self.your_model(self.X_data)\n",
    "\n",
    "        \n",
    "        self.init_summary_op()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def init_summary_op(self):\n",
    "        self.writer_master = tf.summary.FileWriter(self.log_path, self.sess.graph)\n",
    "        self.init_summary=False\n",
    " \n",
    "        self.value_feeder=tf.placeholder(tf.float32)\n",
    "        \n",
    "        \n",
    "    def init_training_parameter(self):\n",
    "        \n",
    "        self.global_step_net= tf.Variable(0, name='global_step_net', trainable=False)\n",
    "        self.dropout_keep_prob_tensor=tf.placeholder(tf.float32)\n",
    "        self.dropout_keep_prob=1.0\n",
    "        self.learning_rate=0.001\n",
    "        self.learning_rate_tensor=tf.placeholder(tf.float32)\n",
    "      \n",
    "        self.global_step=0\n",
    "        self.checkpoints=0\n",
    "\n",
    "        \n",
    "    def init_checking_point_and_log_dir(self,logdir,model_name):\n",
    "        checking_point_and_log=os.path.join(logdir,model_name)\n",
    "        if not os.path.exists(checking_point_and_log):\n",
    "            os.mkdir(checking_point_and_log)\n",
    "            \n",
    "        self.checking_point_path=os.path.join(checking_point_and_log,'model')\n",
    "        if not os.path.exists(self.checking_point_path):\n",
    "            os.mkdir(self.checking_point_path)\n",
    "        self.checking_point_path=os.path.join(self.checking_point_path,model_name)\n",
    "        \n",
    "        self.log_path=os.path.join(checking_point_and_log,'log')\n",
    "        if not os.path.exists(self.log_path):\n",
    "            os.mkdir(self.log_path)\n",
    "        \n",
    "        self.text_report_path=os.path.join(checking_point_and_log,model_name+'.csv')\n",
    "        \n",
    "        \n",
    "    def your_model(self,X_data):\n",
    "\n",
    "        \n",
    "        net,endpoint=alexnet.alexnet_v2(X_data,\n",
    "               num_classes=3,\n",
    "               is_training=True,\n",
    "               dropout_keep_prob=0.7,\n",
    "               spatial_squeeze=False,\n",
    "               scope='alexnet_v2')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.train_loss=slim.losses.softmax_cross_entropy(net,self.y_label)\n",
    "        \n",
    "        optimizer= tf.train.GradientDescentOptimizer(self.learning_rate_tensor)\n",
    "        \n",
    "        self.train_op=slim.learning.create_train_op(self.train_loss, optimizer,self.global_step_net)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.4,allow_growth = True)\n",
    "        gpu_options = tf.GPUOptions(allow_growth = True)\n",
    "        sess=tf.Session(config=tf.ConfigProto(log_device_placement=True,gpu_options=gpu_options))\n",
    "        #sess=tf.Session()\n",
    "\n",
    "        return net,sess \n",
    "\n",
    "        \n",
    "        #return self.train_op\n",
    "    def predict(self,data,label,max_batch=100,accuracy=False):\n",
    "        test_data_length=len(data)\n",
    "        split_batchs=int((test_data_length-0.1)/max_batch)\n",
    "        for start_partition in range(split_batchs+1):\n",
    "            start_index=start_partition*max_batch\n",
    "            prediction_batch=self.sess.run(self.predict_tensor,feed_dict=\n",
    "                                  {self.X_data:data[start_index:(start_index+max_batch)],\n",
    "                                  self.y_label:label[start_index:(start_index+max_batch)],\n",
    "                                  self.dropout_keep_prob_tensor:self.dropout_keep_prob,\n",
    "                                  self.learning_rate_tensor:self.learning_rate\n",
    "                                  })\n",
    "            \n",
    "            prediction_batch=np.asarray(prediction_batch)\n",
    "            if start_partition == 0 :\n",
    "                prediction=prediction_batch\n",
    "            else:\n",
    "                prediction=np.concatenate([prediction,prediction_batch])\n",
    "        \n",
    "        \n",
    "        if accuracy==True:\n",
    "            accuracy=np.mean(np.equal(prediction.argmax(1),label.argmax(1)))\n",
    "            with tf.Session() as sess:\n",
    "                cross_entropy_loss=slim.losses.softmax_cross_entropy(tf.convert_to_tensor(prediction)\n",
    "                                                                     ,tf.convert_to_tensor(label))\n",
    "                cross_entropy_loss_=sess.run(cross_entropy_loss)\n",
    "\n",
    "            return accuracy,cross_entropy_loss_\n",
    "        else:\n",
    "            return prediction.argmax(1)\n",
    "    \n",
    "    def predict_yours(self,data,max_batch=100):\n",
    "        test_data_length=len(data)\n",
    "        split_batchs=int((test_data_length-0.1)/max_batch)\n",
    "        for start_partition in range(split_batchs+1):\n",
    "            start_index=start_partition*max_batch\n",
    "            prediction_batch=self.sess.run(self.predict_tensor,feed_dict=\n",
    "                                  {self.X_data:data[start_index:(start_index+max_batch)],\n",
    "                                  self.dropout_keep_prob_tensor:self.dropout_keep_prob,\n",
    "                                  self.learning_rate_tensor:self.learning_rate\n",
    "                                  })\n",
    "            \n",
    "            prediction_batch=np.asarray(prediction_batch)\n",
    "            if start_partition == 0 :\n",
    "                prediction=prediction_batch\n",
    "            else:\n",
    "                prediction=np.concatenate([prediction,prediction_batch])\n",
    "\n",
    "        return prediction.argmax(1)\n",
    "      \n",
    "   \n",
    "    \n",
    "    def save_checking_point(self):\n",
    "        Saver=tf.train.Saver()\n",
    "        Saver.save(self.sess, self.checking_point_path, global_step=self.checkpoints)\n",
    "        \n",
    "    def restor_checkin_gpoint(self,checking_point_path):\n",
    "        Saver=tf.train.Saver()\n",
    "        Saver.restore(self.sess, checking_point_path)\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "  \n",
    "        \n",
    "    \n",
    "    def write_summary_and_report(self,test_data,test_label):\n",
    "    \n",
    "        loss=self.train_losses_\n",
    "\n",
    "        accuracy,validation_losses_subnets=self.predict(test_data,test_label,accuracy=True)\n",
    "\n",
    "        \n",
    "        report_data=[accuracy,loss,validation_losses_subnets]\n",
    "   \n",
    " \n",
    "        \n",
    "       \n",
    "        if self.init_summary == False:\n",
    "            self.summary_manager=[]\n",
    "            with tf.variable_scope('Report_h') as scope:\n",
    "                \n",
    "                self.summary_manager.append(tf.summary.scalar('Accuracy', self.value_feeder))\n",
    "                self.summary_manager.append(tf.summary.scalar('Loss', self.value_feeder))\n",
    "                self.summary_manager.append(tf.summary.scalar('Validation_losses', self.value_feeder))\n",
    "\n",
    "                       \n",
    "            self.init_summary=True\n",
    "        \n",
    "        for summary_index in range(len(report_data)):\n",
    "          \n",
    "            summary=self.sess.run(self.summary_manager[summary_index],\n",
    "                                  feed_dict={self.value_feeder:report_data[summary_index]})\n",
    "            self.writer_master.add_summary(summary,self.checkpoints)\n",
    "   \n",
    "        self.writer_master.flush()\n",
    "   \n",
    "        \n",
    "        \n",
    "    \n",
    "    def validation(self,test_data,test_label,save=True,init=False):\n",
    "\n",
    "        if save== True:\n",
    "            self.save_checking_point()\n",
    "        self.write_summary_and_report(test_data,test_label)\n",
    "        self.checkpoints+=1\n",
    "        \n",
    "        return 0\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def train(self,X_data,y_label):\n",
    "\n",
    "        \n",
    "        _,self.train_losses_=self.sess.run([\n",
    "                                            self.train_op,self.train_loss],\n",
    "                                                       feed_dict={ \n",
    "                                                       self.X_data:X_data,self.y_label:y_label,\n",
    "                                                      self.dropout_keep_prob_tensor:self.dropout_keep_prob,\n",
    "                                                       self.learning_rate_tensor:self.learning_rate})\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "       \n",
    "        self.global_step+=1\n",
    "        return 1\n",
    "     \n",
    "\n",
    "    def fit(self,train_batch_generater,test_data,test_label,steps,valid_data_per_step,times=1,learning_rate_decay=False,\n",
    "           learning_rate=False,dropout_keep_prob=False):\n",
    "        if learning_rate != False:\n",
    "            if type(learning_rate) is float:\n",
    "                self.learning_rate=learning_rate\n",
    "            else:\n",
    "                raise ValueError(\"learning_rate must be float\")\n",
    "                \n",
    "        if dropout_keep_prob != False:\n",
    "            if type(dropout_keep_prob) is float:\n",
    "                if dropout_keep_prob <= 1.0:\n",
    "                    self.dropout_keep_prob=dropout_keep_prob\n",
    "                else:\n",
    "                    raise ValueError(\"dropout_keep_prob must smaller than or equal to 1.0\")\n",
    "            else:\n",
    "                raise ValueError(\"dropout_keep_prob must be float\")\n",
    " \n",
    "                \n",
    "        \n",
    "        for times_ in list(range(times)):\n",
    "            for step in list(range(steps)):\n",
    "                X_data,y_label=next(train_batch_generater)\n",
    "                #print (self.global_step)\n",
    "                X_data=X_data\n",
    "                #prediction = self.train(X_data,y_label)\n",
    "                self.train(X_data,y_label)\n",
    "                if type(learning_rate_decay) is int:\n",
    "                    if self.global_step % learning_rate_decay == 0 :\n",
    "                        if self.global_step != 0 :\n",
    "                            self.learning_rate=self.learning_rate/2\n",
    "                            \n",
    "                if self.global_step % valid_data_per_step == 0 :\n",
    "                    test_data=test_data\n",
    "                    print (self.global_step)\n",
    "                    _=self.validation(test_data,test_label)\n",
    "                    \n",
    "                    \n",
    "                    ##255\n",
    "                    \n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logdir='/home/Mingke.Chen/DR/log/train'\n",
    "model_name='hson155'\n",
    "filepath='/raidHDD/experimentData/money/upup10'\n",
    "testfile='/raidHDD/experimentData/money/orign/07202215_re'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_generater=batch_data_generater()\n",
    "train_batch=batch_generater.get_batch(filepath,batch_size=10,batch_type='balance')\n",
    "test_X,test_y=next(train_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_file=batch_data_generater_valid()\n",
    "test_batch=test_file.get_batch(testfile,batch_size=310,batch_type='balance')\n",
    "test_X,test_y=next(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_op=VeriSee_training(logdir,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_op.fit(train_batch,test_X,test_y,steps=100000,valid_data_per_step=4000,\n",
    "            times=40,dropout_keep_prob=0.5,learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
